{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexmy/JULIA/SGS/SGS/venv/lib64/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject <SGS_Transformers.RobertaTokenizerWrapper object at 0x7faaabf84370>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyCall\n",
    "using DataFrames\n",
    "\n",
    "# Import the Hugging Face Transformers library\n",
    "transformers    = pyimport(\"transformers\")\n",
    "torch           = pyimport(\"torch\")\n",
    "\n",
    "# Import the fine_tune_model and parse_decoded_strings functions from the Python script\n",
    "py\"\"\"\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from SGS_Transformers import BertTokenizerWrapper, RobertaTokenizerWrapper, GPT2TokenizerWrapper\n",
    "\"\"\"\n",
    "\n",
    "# Define the dataset\n",
    "texts = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"Transformers are amazing!\",\n",
    "    \"Let's tokenize this text.\"\n",
    "]\n",
    "\n",
    "# Create a DataFrame to hold the text data\n",
    "df = DataFrame(text = texts)\n",
    "\n",
    "# Define a function to create batches\n",
    "function create_batches(df, batch_size)\n",
    "    batches = []\n",
    "    for i in 1:batch_size:size(df, 1)\n",
    "        push!(batches, df[i:min(i+batch_size-1, size(df, 1)), :])\n",
    "    end\n",
    "    return batches\n",
    "end\n",
    "\n",
    "# Create batches with a batch size of 2\n",
    "batches = create_batches(df, 2)\n",
    "\n",
    "# Instantiate a tokenizer wrapper (e.g., BERT)\n",
    "tokenizer = py\"RobertaTokenizerWrapper\"()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello, how are you?\n",
      "Tokenized Tokens: [\"Hello\", \",\", \"Ġhow\", \"Ġare\", \"Ġyou\", \"?\"]\n",
      "Original Text: Transformers are amazing!\n",
      "Tokenized Tokens: [\"Transform\", \"ers\", \"Ġare\", \"Ġamazing\", \"!\"]\n",
      "Original Text: Let's tokenize this text.\n",
      "Tokenized Tokens: [\"Let\", \"'s\", \"Ġtoken\", \"ize\", \"Ġthis\", \"Ġtext\", \".\"]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text data using the batches\n",
    "for batch in batches\n",
    "    for text in batch.text\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        println(\"Original Text: $text\")\n",
    "        println(\"Tokenized Tokens: $tokens\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA\n",
    "\n",
    "# Check if the GPU is available\n",
    "if CUDA.has_cuda()\n",
    "    println(\"CUDA is available\")\n",
    "    println(\"Device: \", CUDA.device())\n",
    "else\n",
    "    println(\"CUDA is not available\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA\n",
    "\n",
    "# Example: Move a tensor to the GPU and perform operations\n",
    "function gpu_example()\n",
    "    # Create a random tensor on the CPU\n",
    "    cpu_tensor = rand(Float32, 1000, 1000)\n",
    "    \n",
    "    # Move the tensor to the GPU\n",
    "    gpu_tensor = CUDA.fill(0.0f0, 10000, 10000)\n",
    "    CUDA.copyto!(gpu_tensor, cpu_tensor)\n",
    "    \n",
    "    # Perform operations on the GPU\n",
    "    gpu_result = gpu_tensor .+ 1.0f0\n",
    "    \n",
    "    # Move the result back to the CPU\n",
    "    cpu_result = Array(gpu_result)\n",
    "    \n",
    "    return cpu_result\n",
    "end\n",
    "\n",
    "result = gpu_example()\n",
    "println(\"Result: done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.versioninfo()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
