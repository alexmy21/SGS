{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyObject\n"
     ]
    }
   ],
   "source": [
    "include(\"src/sgs_store.jl\")\n",
    "\n",
    "using ..Entity\n",
    "using ..HllSets\n",
    "\n",
    "using PyCall\n",
    "using CSV\n",
    "using DataFrames\n",
    "using WordTokenizers\n",
    "\n",
    "redis   = pyimport(\"redis\")\n",
    "# Connect to Redis\n",
    "r = redis.Redis(host=\"localhost\", port=6379, db=0)\n",
    "\n",
    "csv_file_path = \"/home/alexmy/Downloads/POC/DATA/enron_05_17_2015_with_labels_v2.csv\"\n",
    "\n",
    "println(typeof(r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df      = DataFrame(CSV.File(csv_file_path, header=true))\n",
    "df.From = map(x -> ismissing(x) ? \"\" : (isnothing(match(r\"'([^']*)'\", x)) ? \"\" : match(r\"'([^']*)'\", x).captures[1]), df.From)\n",
    "df.To   = map(x -> ismissing(x) ? \"\" : (isnothing(match(r\"'([^']*)'\", x)) ? \"\" : match(r\"'([^']*)'\", x).captures[1]), df.To)\n",
    "\n",
    "df_filled = coalesce.(df, \"unknown\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HllSet{10}()\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    1. Message-ID\t\n",
    "    2. Date\t\n",
    "    3. From\t\n",
    "    4. To\t\n",
    "    5. Subject\t\n",
    "    6. X-From\tX-To\tX-cc\tX-bcc\tX-Folder\tX-Origin\tX-FileName\t\n",
    "    13. content\t\n",
    "    14. user\t\n",
    "    15. Cat_1_level_1\tCat_1_level_2\tCat_1_weight\tCat_2_level_1\tCat_2_level_2\tCat_2_weight\tCat_3_level_1\tCat_3_level_2\tCat_3_weight\tCat_4_level_1\tCat_4_level_2\tCat_4_weight\tCat_5_level_1\tCat_5_level_2\tCat_5_weight\tCat_6_level_1\tCat_6_level_2\tCat_6_weight\tCat_7_level_1\tCat_7_level_2\tCat_7_weight\tCat_8_level_1\tCat_8_level_2\tCat_8_weight\tCat_9_level_1\tCat_9_level_2\tCat_9_weight\tCat_10_level_1\tCat_10_level_2\tCat_10_weight\tCat_11_level_1\tCat_11_level_2\tCat_11_weight\tCat_12_level_1\tCat_12_level_2\tCat_12_weight\t\n",
    "    51. labeled\n",
    "\"\"\"\n",
    "\n",
    "# Initialize sets to collect data from specific columns\n",
    "hll_03 = HllSets.HllSet{10}()     # Set{String}()\n",
    "hll_04 = HllSets.HllSet{10}()     # Set{String}()\n",
    "hll_05 = HllSets.HllSet{10}()     # Set{String}()\n",
    "hll_06 = HllSets.HllSet{10}()     # Set{String}()\n",
    "hll_14 = HllSets.HllSet{10}()     # Set{String}()\n",
    "hll_15 = HllSets.HllSet{10}()     # Set{String}()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6f983ba3758e7233f7379a9c7b6ee565808a8de6; num_chunks: 79\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching store_entity(::PyObject, ::Int64, ::HllSet{10}; grad::Float64, op::Nothing, prefix::String)\n\n\u001b[0mClosest candidates are:\n\u001b[0m  store_entity(::PyObject, \u001b[91m::String\u001b[39m, \u001b[91m::String\u001b[39m; grad, op, prefix)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mMain.Entity\u001b[39m \u001b[90m~/JULIA/SGS/SGS/src/\u001b[39m\u001b[90m\u001b[4msgs_entity.jl:49\u001b[24m\u001b[39m\n\u001b[0m  store_entity(::PyObject, \u001b[91m::HllSet{P}\u001b[39m; grad, op, prefix) where P\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mMain.Entity\u001b[39m \u001b[90m~/JULIA/SGS/SGS/src/\u001b[39m\u001b[90m\u001b[4msgs_entity.jl:41\u001b[24m\u001b[39m\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching store_entity(::PyObject, ::Int64, ::HllSet{10}; grad::Float64, op::Nothing, prefix::String)\n\n\u001b[0mClosest candidates are:\n\u001b[0m  store_entity(::PyObject, \u001b[91m::String\u001b[39m, \u001b[91m::String\u001b[39m; grad, op, prefix)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mMain.Entity\u001b[39m \u001b[90m~/JULIA/SGS/SGS/src/\u001b[39m\u001b[90m\u001b[4msgs_entity.jl:49\u001b[24m\u001b[39m\n\u001b[0m  store_entity(::PyObject, \u001b[91m::HllSet{P}\u001b[39m; grad, op, prefix) where P\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mMain.Entity\u001b[39m \u001b[90m~/JULIA/SGS/SGS/src/\u001b[39m\u001b[90m\u001b[4msgs_entity.jl:41\u001b[24m\u001b[39m\n",
      "",
      "Stacktrace:",
      " [1] Instance{10}(r::PyObject, hll::HllSet{10}; grad::Float64, op::Nothing, prefix::String)",
      "   @ Main.Entity ~/JULIA/SGS/SGS/src/sgs_entity.jl:102",
      " [2] Instance{10}(r::PyObject, hll::HllSet{10})",
      "   @ Main.Entity ~/JULIA/SGS/SGS/src/sgs_entity.jl:100",
      " [3] top-level scope",
      "   @ ./In[4]:32"
     ]
    }
   ],
   "source": [
    "using ..Store\n",
    "using ..Util\n",
    "using JSON3\n",
    "using TextAnalysis\n",
    "using WordTokenizers\n",
    "using Base.Threads\n",
    "\n",
    "tokenizer = WordTokenizers.Words\n",
    "# r::PyObject, df::DataFrame, parent::String, cols::Vector; p::Int=10, chunk_size::Int=512000\n",
    "# Store.ingest_df(r, tokenizer, df, csv_file_path, [:From, :To, :Subject, :content, :user])\n",
    "\n",
    "cols = [:From, :To, :Subject, :content, :user]\n",
    "p::Int=10 \n",
    "chunk_size::Int=512000\n",
    "parent = csv_file_path\n",
    "\n",
    "for column in cols    \n",
    "    col_values  = df[:, column]\n",
    "    col_sha1    = Util.sha1_union([parent, string(column)])\n",
    "    column_size = Base.summarysize(col_values)\n",
    "    num_chunks  = ceil(Int, column_size / chunk_size)\n",
    "    chunks      = Store.chunk_array(col_values, num_chunks)\n",
    "\n",
    "    println(col_sha1, \"; num_chunks: \", num_chunks)\n",
    "    dataset = Store.ingest_df_column(r, tokenizer, chunks, col_sha1)\n",
    "    # println(dataset)\n",
    "    hll = HllSets.HllSet{10}()\n",
    "    # println(hll)\n",
    "    dataset = JSON3.write(dataset)\n",
    "    hll = HllSets.restore(hll, dataset)\n",
    "    # println(hll)\n",
    "    entity = Entity.Instance{10}(r, hll)\n",
    "    println(\"Column entity instance: \", entity)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using Distributions\n",
    "\n",
    "# Example term frequencies (tf)\n",
    "terms = [\"term1\", \"term2\", \"term3\", \"term4\", \"term5\"]\n",
    "tf = [10, 20, 30, 25, 15]\n",
    "\n",
    "# Calculate the total term frequency\n",
    "total_tf = sum(tf)\n",
    "\n",
    "# Calculate the probability distribution\n",
    "probabilities = tf ./ total_tf\n",
    "\n",
    "# Create a categorical distribution based on the probabilities\n",
    "term_distribution = Categorical(probabilities)\n",
    "\n",
    "# Function to sample terms based on the distribution\n",
    "function sample_terms(terms, term_distribution, num_samples)\n",
    "    sampled_terms = []\n",
    "    for _ in 1:num_samples\n",
    "        term_index = rand(term_distribution)\n",
    "        push!(sampled_terms, terms[term_index])\n",
    "    end\n",
    "    return sampled_terms\n",
    "end\n",
    "\n",
    "# Sample 10 terms from the collection\n",
    "num_samples = 3\n",
    "sampled_terms = sample_terms(terms, term_distribution, num_samples)\n",
    "\n",
    "println(\"Sampled terms: \", sampled_terms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
