{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyObject\n"
     ]
    }
   ],
   "source": [
    "include(\"src/sgs_store.jl\")\n",
    "\n",
    "using ..Entity\n",
    "using ..HllSets\n",
    "\n",
    "using PyCall\n",
    "using CSV\n",
    "using DataFrames\n",
    "using WordTokenizers\n",
    "\n",
    "redis   = pyimport(\"redis\")\n",
    "# Connect to Redis\n",
    "r = redis.Redis(host=\"localhost\", port=6379, db=0)\n",
    "\n",
    "csv_file_path = \"/home/alexmy/Downloads/POC/DATA/enron_05_17_2015_with_labels_v2.csv\"\n",
    "\n",
    "println(typeof(r))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting tokens by applying multiple filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "PyError ($(Expr(:escape, :(ccall(#= /home/alexmy/.julia/packages/PyCall/1gn3u/src/pyfncall.jl:43 =# @pysym(:PyObject_Call), PyPtr, (PyPtr, PyPtr, PyPtr), o, pyargsptr, kw))))) <class 'redis.exceptions.ResponseError'>\nResponseError('Function not found')\n  File \"/home/alexmy/JULIA/SGS/SGS/venv/lib64/python3.10/site-packages/redis/commands/core.py\", line 6421, in fcall\n    return self._fcall(\"FCALL\", function, numkeys, *keys_and_args)\n  File \"/home/alexmy/JULIA/SGS/SGS/venv/lib64/python3.10/site-packages/redis/commands/core.py\", line 6411, in _fcall\n    return self.execute_command(command, function, numkeys, *keys_and_args)\n  File \"/home/alexmy/JULIA/SGS/SGS/venv/lib64/python3.10/site-packages/redis/client.py\", line 548, in execute_command\n    return conn.retry.call_with_retry(\n  File \"/home/alexmy/JULIA/SGS/SGS/venv/lib64/python3.10/site-packages/redis/retry.py\", line 62, in call_with_retry\n    return do()\n  File \"/home/alexmy/JULIA/SGS/SGS/venv/lib64/python3.10/site-packages/redis/client.py\", line 549, in <lambda>\n    lambda: self._send_command_parse_response(\n  File \"/home/alexmy/JULIA/SGS/SGS/venv/lib64/python3.10/site-packages/redis/client.py\", line 525, in _send_command_parse_response\n    return self.parse_response(conn, command_name, **options)\n  File \"/home/alexmy/JULIA/SGS/SGS/venv/lib64/python3.10/site-packages/redis/client.py\", line 565, in parse_response\n    response = connection.read_response()\n  File \"/home/alexmy/JULIA/SGS/SGS/venv/lib64/python3.10/site-packages/redis/connection.py\", line 536, in read_response\n    raise response\n",
     "output_type": "error",
     "traceback": [
      "PyError ($(Expr(:escape, :(ccall(#= /home/alexmy/.julia/packages/PyCall/1gn3u/src/pyfncall.jl:43 =# @pysym(:PyObject_Call), PyPtr, (PyPtr, PyPtr, PyPtr), o, pyargsptr, kw))))) <class 'redis.exceptions.ResponseError'>\nResponseError('Function not found')\n  File \"/home/alexmy/JULIA/SGS/SGS/venv/lib64/python3.10/site-packages/redis/commands/core.py\", line 6421, in fcall\n    return self._fcall(\"FCALL\", function, numkeys, *keys_and_args)\n  File \"/home/alexmy/JULIA/SGS/SGS/venv/lib64/python3.10/site-packages/redis/commands/core.py\", line 6411, in _fcall\n    return self.execute_command(command, function, numkeys, *keys_and_args)\n  File \"/home/alexmy/JULIA/SGS/SGS/venv/lib64/python3.10/site-packages/redis/client.py\", line 548, in execute_command\n    return conn.retry.call_with_retry(\n  File \"/home/alexmy/JULIA/SGS/SGS/venv/lib64/python3.10/site-packages/redis/retry.py\", line 62, in call_with_retry\n    return do()\n  File \"/home/alexmy/JULIA/SGS/SGS/venv/lib64/python3.10/site-packages/redis/client.py\", line 549, in <lambda>\n    lambda: self._send_command_parse_response(\n  File \"/home/alexmy/JULIA/SGS/SGS/venv/lib64/python3.10/site-packages/redis/client.py\", line 525, in _send_command_parse_response\n    return self.parse_response(conn, command_name, **options)\n  File \"/home/alexmy/JULIA/SGS/SGS/venv/lib64/python3.10/site-packages/redis/client.py\", line 565, in parse_response\n    response = connection.read_response()\n  File \"/home/alexmy/JULIA/SGS/SGS/venv/lib64/python3.10/site-packages/redis/connection.py\", line 536, in read_response\n    raise response\n",
      "",
      "Stacktrace:",
      "  [1] pyerr_check",
      "    @ ~/.julia/packages/PyCall/1gn3u/src/exception.jl:75 [inlined]",
      "  [2] pyerr_check",
      "    @ ~/.julia/packages/PyCall/1gn3u/src/exception.jl:79 [inlined]",
      "  [3] _handle_error(msg::String)",
      "    @ PyCall ~/.julia/packages/PyCall/1gn3u/src/exception.jl:96",
      "  [4] macro expansion",
      "    @ ~/.julia/packages/PyCall/1gn3u/src/exception.jl:110 [inlined]",
      "  [5] #107",
      "    @ ~/.julia/packages/PyCall/1gn3u/src/pyfncall.jl:43 [inlined]",
      "  [6] disable_sigint",
      "    @ ./c.jl:473 [inlined]",
      "  [7] __pycall!",
      "    @ ~/.julia/packages/PyCall/1gn3u/src/pyfncall.jl:42 [inlined]",
      "  [8] _pycall!(ret::PyObject, o::PyObject, args::Tuple{String, Int64, String, String, String, String, String, String, Int64}, nargs::Int64, kw::Ptr{Nothing})",
      "    @ PyCall ~/.julia/packages/PyCall/1gn3u/src/pyfncall.jl:29",
      "  [9] _pycall!(ret::PyObject, o::PyObject, args::Tuple{String, Int64, String, String, String, String, String, String, Int64}, kwargs::@Kwargs{})",
      "    @ PyCall ~/.julia/packages/PyCall/1gn3u/src/pyfncall.jl:11",
      " [10] (::PyObject)(::String, ::Vararg{Any}; kwargs::@Kwargs{})",
      "    @ PyCall ~/.julia/packages/PyCall/1gn3u/src/pyfncall.jl:86",
      " [11] top-level scope",
      "    @ In[4]:1"
     ]
    }
   ],
   "source": [
    "matrix = r.fcall(\"retrieve_tokens\", 1, \"t:\", \"refs\", \"match\", \"*\", \"tf\", \">\", 4)\n",
    "# println(matrix[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function generate_overlapping_sets(column, N)\n",
    "    sets = []\n",
    "    for i in 1:(length(column) - N + 1)\n",
    "        push!(sets, column[i:(i + N - 1)])\n",
    "    end\n",
    "    return sets\n",
    "end\n",
    "\n",
    "first_column = matrix[:, 1]\n",
    "N = 11\n",
    "\n",
    "overlapping_sets = generate_overlapping_sets(first_column, N)\n",
    "\n",
    "# Print the overlapping sets\n",
    "for set in overlapping_sets\n",
    "    println(set)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading CSV file to DataFrame and formatting some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df      = DataFrame(CSV.File(csv_file_path, header=true))\n",
    "df.From = map(x -> ismissing(x) ? \"\" : (isnothing(match(r\"'([^']*)'\", x)) ? \"\" : match(r\"'([^']*)'\", x).captures[1]), df.From)\n",
    "df.To   = map(x -> ismissing(x) ? \"\" : (isnothing(match(r\"'([^']*)'\", x)) ? \"\" : match(r\"'([^']*)'\", x).captures[1]), df.To)\n",
    "\n",
    "df_filled = coalesce.(df, \"unknown\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    1. Message-ID\t\n",
    "    2. Date\t\n",
    "    3. From\t\n",
    "    4. To\t\n",
    "    5. Subject\t\n",
    "    6. X-From\tX-To\tX-cc\tX-bcc\tX-Folder\tX-Origin\tX-FileName\t\n",
    "    13. content\t\n",
    "    14. user\t\n",
    "    15. Cat_1_level_1\tCat_1_level_2\tCat_1_weight\tCat_2_level_1\tCat_2_level_2\tCat_2_weight\tCat_3_level_1\tCat_3_level_2\tCat_3_weight\tCat_4_level_1\tCat_4_level_2\tCat_4_weight\tCat_5_level_1\tCat_5_level_2\tCat_5_weight\tCat_6_level_1\tCat_6_level_2\tCat_6_weight\tCat_7_level_1\tCat_7_level_2\tCat_7_weight\tCat_8_level_1\tCat_8_level_2\tCat_8_weight\tCat_9_level_1\tCat_9_level_2\tCat_9_weight\tCat_10_level_1\tCat_10_level_2\tCat_10_weight\tCat_11_level_1\tCat_11_level_2\tCat_11_weight\tCat_12_level_1\tCat_12_level_2\tCat_12_weight\t\n",
    "    51. labeled\n",
    "\"\"\"\n",
    "\n",
    "# Initialize sets to collect data from specific columns\n",
    "hll_03 = HllSets.HllSet{10}()     # Set{String}()\n",
    "hll_04 = HllSets.HllSet{10}()     # Set{String}()\n",
    "hll_05 = HllSets.HllSet{10}()     # Set{String}()\n",
    "hll_06 = HllSets.HllSet{10}()     # Set{String}()\n",
    "hll_14 = HllSets.HllSet{10}()     # Set{String}()\n",
    "hll_15 = HllSets.HllSet{10}()     # Set{String}()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ..Store\n",
    "using ..Util\n",
    "using JSON3\n",
    "using TextAnalysis\n",
    "using WordTokenizers\n",
    "using Base.Threads\n",
    "\n",
    "tokenizer = WordTokenizers.Words\n",
    "# r::PyObject, df::DataFrame, parent::String, cols::Vector; p::Int=10, chunk_size::Int=512000\n",
    "# Store.ingest_df(r, tokenizer, df, csv_file_path, [:From, :To, :Subject, :content, :user])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting tokens by column and saving processed columns as hash in Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [:From, :To, :Subject, :content, :user]\n",
    "p::Int=10 \n",
    "chunk_size::Int=512000\n",
    "_parent = csv_file_path\n",
    "\n",
    "for column in cols    \n",
    "    col_values  = df[:, column]\n",
    "    col_sha1    = Util.sha1_union([_parent, string(column)])\n",
    "    column_size = Base.summarysize(col_values)\n",
    "    num_chunks  = ceil(Int, column_size / chunk_size)\n",
    "    chunks      = Store.chunk_array(col_values, num_chunks)\n",
    "\n",
    "    println(col_sha1, \"; num_chunks: \", num_chunks)\n",
    "    dataset = Store.ingest_df_column(r, tokenizer, chunks, col_sha1)\n",
    "    # println(dataset)\n",
    "    hll = HllSets.HllSet{10}()\n",
    "    # println(hll)\n",
    "    dataset = JSON3.write(dataset)\n",
    "    hll = HllSets.restore(hll, dataset)\n",
    "    # println(hll)\n",
    "    entity = Entity.Instance{10}(r, hll)\n",
    "    println(\"Column entity instance: \", entity)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building sample set of known size (cardinality) from categorical distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using Distributions\n",
    "\n",
    "# Example term frequencies (tf)\n",
    "terms = [\"term1\", \"term2\", \"term3\", \"term4\", \"term5\"]\n",
    "tf = [10, 20, 30, 25, 15]\n",
    "\n",
    "# Calculate the total term frequency\n",
    "total_tf = sum(tf)\n",
    "\n",
    "# Calculate the probability distribution\n",
    "probabilities = tf ./ total_tf\n",
    "\n",
    "# Create a categorical distribution based on the probabilities\n",
    "term_distribution = Categorical(probabilities)\n",
    "\n",
    "# Function to sample terms based on the distribution\n",
    "function sample_terms(terms, term_distribution, num_samples)\n",
    "    sampled_terms = []\n",
    "    for _ in 1:num_samples\n",
    "        term_index = rand(term_distribution)\n",
    "        push!(sampled_terms, terms[term_index])\n",
    "    end\n",
    "    return sampled_terms\n",
    "end\n",
    "\n",
    "# Sample 10 terms from the collection\n",
    "num_samples = 3\n",
    "sampled_terms = sample_terms(terms, term_distribution, num_samples)\n",
    "\n",
    "println(\"Sampled terms: \", sampled_terms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
