{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyObject\n"
     ]
    }
   ],
   "source": [
    "include(\"src/sgs_store.jl\")\n",
    "\n",
    "using ..Entity\n",
    "using ..HllSets\n",
    "\n",
    "using PyCall\n",
    "using CSV\n",
    "using DataFrames\n",
    "using WordTokenizers\n",
    "\n",
    "redis   = pyimport(\"redis\")\n",
    "# Connect to Redis\n",
    "r = redis.Redis(host=\"localhost\", port=6379, db=0)\n",
    "\n",
    "csv_file_path = \"/home/alexmy/Downloads/POC/DATA/enron_05_17_2015_with_labels_v2.csv\"\n",
    "\n",
    "println(typeof(r))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting tokens by applying multiple filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17182×2 Matrix{String}:\n",
       " \"t:2770891406\"  \"130\"\n",
       " \"t:1315446192\"  \"8\"\n",
       " \"t:3432805380\"  \"15\"\n",
       " \"t:2836343235\"  \"43\"\n",
       " \"t:285191118\"   \"9\"\n",
       " \"t:2771454975\"  \"19\"\n",
       " \"t:1741283176\"  \"6\"\n",
       " \"t:260882023\"   \"13\"\n",
       " \"t:413436136\"   \"13\"\n",
       " \"t:3755412643\"  \"46\"\n",
       " \"t:3405800736\"  \"47\"\n",
       " \"t:3830922237\"  \"59\"\n",
       " \"t:3888023939\"  \"608\"\n",
       " ⋮               \n",
       " \"t:556260589\"   \"12\"\n",
       " \"t:1002174625\"  \"29\"\n",
       " \"t:3159625224\"  \"8\"\n",
       " \"t:3581244060\"  \"5\"\n",
       " \"t:3562180998\"  \"11\"\n",
       " \"t:1390805360\"  \"31\"\n",
       " \"t:2713214389\"  \"5\"\n",
       " \"t:3660487975\"  \"20\"\n",
       " \"t:4122236966\"  \"8\"\n",
       " \"t:1007943058\"  \"5\"\n",
       " \"t:1781610051\"  \"6\"\n",
       " \"t:2832069688\"  \"40\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = r.fcall(\"retrieve_tokens\", 1, \"t:\", \"refs\", \"match\", \"6f983ba3758e7233f7379a9c7b6ee565808a8de6 6bc47f481f9b458cf32e52dbd4d6731a5d198af5\", \"tf\", \">\", 4)\n",
    "# println(matrix[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function generate_overlapping_sets(column, N)\n",
    "    sets = []\n",
    "    for i in 1:(length(column) - N + 1)\n",
    "        push!(sets, column[i:(i + N - 1)])\n",
    "    end\n",
    "    return sets\n",
    "end\n",
    "\n",
    "first_column = matrix[:, 1]\n",
    "N = 11\n",
    "\n",
    "overlapping_sets = generate_overlapping_sets(first_column, N)\n",
    "\n",
    "# Print the overlapping sets\n",
    "for set in overlapping_sets\n",
    "    println(set)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading CSV file to DataFrame and formatting some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df      = DataFrame(CSV.File(csv_file_path, header=true))\n",
    "df.From = map(x -> ismissing(x) ? \"\" : (isnothing(match(r\"'([^']*)'\", x)) ? \"\" : match(r\"'([^']*)'\", x).captures[1]), df.From)\n",
    "df.To   = map(x -> ismissing(x) ? \"\" : (isnothing(match(r\"'([^']*)'\", x)) ? \"\" : match(r\"'([^']*)'\", x).captures[1]), df.To)\n",
    "\n",
    "df_filled = coalesce.(df, \"unknown\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HllSet{10}()\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    1. Message-ID\t\n",
    "    2. Date\t\n",
    "    3. From\t\n",
    "    4. To\t\n",
    "    5. Subject\t\n",
    "    6. X-From\tX-To\tX-cc\tX-bcc\tX-Folder\tX-Origin\tX-FileName\t\n",
    "    13. content\t\n",
    "    14. user\t\n",
    "    15. Cat_1_level_1\tCat_1_level_2\tCat_1_weight\tCat_2_level_1\tCat_2_level_2\tCat_2_weight\tCat_3_level_1\tCat_3_level_2\tCat_3_weight\tCat_4_level_1\tCat_4_level_2\tCat_4_weight\tCat_5_level_1\tCat_5_level_2\tCat_5_weight\tCat_6_level_1\tCat_6_level_2\tCat_6_weight\tCat_7_level_1\tCat_7_level_2\tCat_7_weight\tCat_8_level_1\tCat_8_level_2\tCat_8_weight\tCat_9_level_1\tCat_9_level_2\tCat_9_weight\tCat_10_level_1\tCat_10_level_2\tCat_10_weight\tCat_11_level_1\tCat_11_level_2\tCat_11_weight\tCat_12_level_1\tCat_12_level_2\tCat_12_weight\t\n",
    "    51. labeled\n",
    "\"\"\"\n",
    "\n",
    "# Initialize sets to collect data from specific columns\n",
    "hll_03 = HllSets.HllSet{10}()     # Set{String}()\n",
    "hll_04 = HllSets.HllSet{10}()     # Set{String}()\n",
    "hll_05 = HllSets.HllSet{10}()     # Set{String}()\n",
    "hll_06 = HllSets.HllSet{10}()     # Set{String}()\n",
    "hll_14 = HllSets.HllSet{10}()     # Set{String}()\n",
    "hll_15 = HllSets.HllSet{10}()     # Set{String}()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenize (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ..Store\n",
    "using ..Util\n",
    "using JSON3\n",
    "using TextAnalysis\n",
    "using WordTokenizers\n",
    "using Base.Threads\n",
    "\n",
    "tokenizer = WordTokenizers.Words\n",
    "# r::PyObject, df::DataFrame, parent::String, cols::Vector; p::Int=10, chunk_size::Int=512000\n",
    "# Store.ingest_df(r, tokenizer, df, csv_file_path, [:From, :To, :Subject, :content, :user])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting tokens by column and saving processed columns as hash in Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6f983ba3758e7233f7379a9c7b6ee565808a8de6; num_chunks: 79\n",
      "Column entity instance: \n",
      "Instance(\n",
      " sha1: 6f0a114f998daf428bed86d527be629e2cefe8bc\n",
      " card: 12173\n",
      " hll: HllSet{10}()\n",
      " grad: 0.0\n",
      " op: nothing)\n",
      "\n",
      "\n",
      "6bc47f481f9b458cf32e52dbd4d6731a5d198af5; num_chunks: 209\n",
      "Column entity instance: \n",
      "Instance(\n",
      " sha1: 3a577c261dc685162db75d6c025fc0fdaf85bf42\n",
      " card: 10511\n",
      " hll: HllSet{10}()\n",
      " grad: 0.0\n",
      " op: nothing)\n",
      "\n",
      "\n",
      "f6c9fedfe796b71638efc125e924040013ef5234; num_chunks: 45\n",
      "Column entity instance: \n",
      "Instance(\n",
      " sha1: f92388e6781f4bdf4a9e24e131090233f3bc0dbd\n",
      " card: 25811\n",
      " hll: HllSet{10}()\n",
      " grad: 0.0\n",
      " op: nothing)\n",
      "\n",
      "\n",
      "65875368cc6392683f42a0e2938b5c0789485b97; num_chunks: 1685\n",
      "Column entity instance: \n",
      "Instance(\n",
      " sha1: e3ade37139ed79dc97bb646f735bca994fade985\n",
      " card: 378258\n",
      " hll: HllSet{10}()\n",
      " grad: 0.0\n",
      " op: nothing)\n",
      "\n",
      "\n",
      "981f459d81197edf542958361ef219372da6bd82; num_chunks: 5\n",
      "Column entity instance: \n",
      "Instance(\n",
      " sha1: 4af3a15f54c2330a7b2e3fe88fc234cfc1dbbbb4\n",
      " card: 49\n",
      " hll: HllSet{10}()\n",
      " grad: 0.0\n",
      " op: nothing)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = [:From, :To, :Subject, :content, :user]\n",
    "p::Int=10 \n",
    "chunk_size::Int=512000\n",
    "_parent = csv_file_path\n",
    "\n",
    "for column in cols    \n",
    "    col_values  = df[:, column]\n",
    "    col_sha1    = Util.sha1_union([_parent, string(column)])\n",
    "    column_size = Base.summarysize(col_values)\n",
    "    num_chunks  = ceil(Int, column_size / chunk_size)\n",
    "    chunks      = Store.chunk_array(col_values, num_chunks)\n",
    "\n",
    "    println(col_sha1, \"; num_chunks: \", num_chunks)\n",
    "    dataset = Store.ingest_df_column(r, tokenizer, chunks, col_sha1)\n",
    "    # println(dataset)\n",
    "    hll = HllSets.HllSet{10}()\n",
    "    # println(hll)\n",
    "    dataset = JSON3.write(dataset)\n",
    "    hll = HllSets.restore(hll, dataset)\n",
    "    # println(hll)\n",
    "    entity = Entity.Instance{10}(r, hll)\n",
    "    println(\"Column entity instance: \", entity)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building sample set of known size (cardinality) from categorical distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using Distributions\n",
    "\n",
    "# Example term frequencies (tf)\n",
    "terms = [\"term1\", \"term2\", \"term3\", \"term4\", \"term5\"]\n",
    "tf = [10, 20, 30, 25, 15]\n",
    "\n",
    "# Calculate the total term frequency\n",
    "total_tf = sum(tf)\n",
    "\n",
    "# Calculate the probability distribution\n",
    "probabilities = tf ./ total_tf\n",
    "\n",
    "# Create a categorical distribution based on the probabilities\n",
    "term_distribution = Categorical(probabilities)\n",
    "\n",
    "# Function to sample terms based on the distribution\n",
    "function sample_terms(terms, term_distribution, num_samples)\n",
    "    sampled_terms = []\n",
    "    for _ in 1:num_samples\n",
    "        term_index = rand(term_distribution)\n",
    "        push!(sampled_terms, terms[term_index])\n",
    "    end\n",
    "    return sampled_terms\n",
    "end\n",
    "\n",
    "# Sample 10 terms from the collection\n",
    "num_samples = 3\n",
    "sampled_terms = sample_terms(terms, term_distribution, num_samples)\n",
    "\n",
    "println(\"Sampled terms: \", sampled_terms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
