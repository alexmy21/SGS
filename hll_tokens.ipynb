{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"src//entity.jl\")\n",
    "using ..Entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate text starting from a seed token\n",
    "seed_token = \"concept\"\n",
    "generated_text = generate_text(seed_token, 50)\n",
    "println(\"Generated Text: $generated_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of tokens and bins\n",
    "num_tokens = 1000\n",
    "num_bins = 1024\n",
    "\n",
    "# Create arrays for each property\n",
    "hash_codes = Vector{UInt32}(undef, num_tokens)\n",
    "entity_instances = [Vector{Entity.Instance{10}}() for _ in 1:num_tokens]\n",
    "tf_frequencies = Vector{UInt32}(undef, num_tokens)\n",
    "bin_numbers = Vector{UInt16}(undef, num_tokens)\n",
    "trailing_zeros = Vector{UInt8}(undef, num_tokens)\n",
    "preceding_tokens = [Vector{UInt32}() for _ in 1:num_tokens]\n",
    "following_tokens = [Vector{UInt32}() for _ in 1:num_tokens]\n",
    "commit_ids = Vector{String}(undef, num_tokens)\n",
    "\n",
    "# Initialize the arrays with random or default values\n",
    "for i in 1:num_tokens\n",
    "    hash_codes[i] = rand(UInt32)\n",
    "    tf_frequencies[i] = rand(UInt32)\n",
    "    bin_numbers[i] = rand(UInt16) % num_bins + 1  # Ensure bin numbers are within the range 1 to num_bins\n",
    "    trailing_zeros[i] = rand(UInt8)\n",
    "    commit_ids[i] = \"\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split tensors by bin number\n",
    "function split_by_bin_number(bin_numbers, num_bins)\n",
    "    bins = [Vector{Int}() for _ in 1:num_bins]\n",
    "    for i in 1:length(bin_numbers)\n",
    "        bin = bin_numbers[i]\n",
    "        push!(bins[bin], i)\n",
    "    end\n",
    "    return bins\n",
    "end\n",
    "\n",
    "# Split the tokens by bin number\n",
    "bins = split_by_bin_number(bin_numbers, num_bins)\n",
    "\n",
    "# Access tokens in a specific bin\n",
    "bin_index = 2\n",
    "tokens_in_bin = bins[bin_index]\n",
    "\n",
    "# Example: Print hash codes of tokens in the specified bin\n",
    "println(\"Hash codes of tokens in bin $bin_index:\")\n",
    "for token_index in tokens_in_bin\n",
    "    println(hash_codes[token_index])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"All token hashes:\")\n",
    "for hash_code in hash_codes\n",
    "    println(hash_code)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hash code of the token you want to retrieve\n",
    "target_hash_code = 2783398889\n",
    "\n",
    "# Find the index of the token with the specified hash code\n",
    "token_index = findfirst(x -> x == target_hash_code, hash_codes)\n",
    "\n",
    "if token_index !== nothing\n",
    "    # Retrieve all properties using the index\n",
    "    token_hash_code = hash_codes[token_index]\n",
    "    token_entity_instances = entity_instances[token_index]\n",
    "    token_tf_frequency = tf_frequencies[token_index]\n",
    "    token_bin_number = bin_numbers[token_index]\n",
    "    token_trailing_zeros = trailing_zeros[token_index]\n",
    "    token_preceding_tokens = preceding_tokens[token_index]\n",
    "    token_following_tokens = following_tokens[token_index]\n",
    "    token_commit_id = commit_ids[token_index]\n",
    "\n",
    "    # Print all properties of the token\n",
    "    println(\"Properties of token with hash code $target_hash_code:\")\n",
    "    println(\"Hash Code: $token_hash_code\")\n",
    "    println(\"Entity Instances: $token_entity_instances\")\n",
    "    println(\"TF Frequency: $token_tf_frequency\")\n",
    "    println(\"Bin Number: $token_bin_number\")\n",
    "    println(\"Trailing Zeros: $token_trailing_zeros\")\n",
    "    println(\"Preceding Tokens: $token_preceding_tokens\")\n",
    "    println(\"Following Tokens: $token_following_tokens\")\n",
    "    println(\"Commit ID: $token_commit_id\")\n",
    "else\n",
    "    println(\"Token with hash code $target_hash_code not found.\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SHA, Random, StatsBase\n",
    "\n",
    "# Define the Token struct\n",
    "struct Token\n",
    "    hash_code::UInt32\n",
    "    token::String\n",
    "    entity_instances::Vector{Entity.Instance{10}}\n",
    "    tf_frequency::Float64\n",
    "    bin_number::UInt16\n",
    "    trailing_zeros::Int\n",
    "    preceding_tokens::Vector{UInt32}\n",
    "    following_tokens::Vector{UInt32}\n",
    "    commit_id::String\n",
    "end\n",
    "\n",
    "# Define the TokenContext struct\n",
    "struct TokenContext\n",
    "    recent_tokens::Vector{UInt32}\n",
    "    max_length::Int\n",
    "end\n",
    "\n",
    "# Initialize a dictionary to store tokens by their hash codes\n",
    "tokens_dict = Dict{UInt32, Token}()\n",
    "\n",
    "# Function to tokenize the text\n",
    "function tokenize(text::String)\n",
    "    return split(text, r\"\\W+\")\n",
    "end\n",
    "\n",
    "# Function to convert a token to a hash\n",
    "function token_to_hash(token::AbstractString)\n",
    "    return reinterpret(UInt32, sha1(String(token))[1:4])[1]\n",
    "end\n",
    "\n",
    "# Function to process the text\n",
    "function process_text(text::String)\n",
    "    tokens = tokenize(text)\n",
    "    num_tokens = length(tokens)\n",
    "    \n",
    "    for i in 1:num_tokens\n",
    "        token = tokens[i]\n",
    "        token_hash = token_to_hash(token)\n",
    "        \n",
    "        if haskey(tokens_dict, token_hash)\n",
    "            # Token already exists, update properties\n",
    "            existing_token = tokens_dict[token_hash]\n",
    "            if i > 1\n",
    "                preceding_token_hash = token_to_hash(tokens[i-1])\n",
    "                push!(existing_token.preceding_tokens, preceding_token_hash)\n",
    "            end\n",
    "            if i < num_tokens\n",
    "                following_token_hash = token_to_hash(tokens[i+1])\n",
    "                push!(existing_token.following_tokens, following_token_hash)\n",
    "            end\n",
    "        else\n",
    "            # Token does not exist, create a new instance\n",
    "            new_token = Token(\n",
    "                token_hash,\n",
    "                token,\n",
    "                Vector{Entity.Instance{10}}(),\n",
    "                1.0,\n",
    "                rand(UInt16),\n",
    "                0,\n",
    "                i > 1 ? [token_to_hash(tokens[i-1])] : [],\n",
    "                i < num_tokens ? [token_to_hash(tokens[i+1])] : [],\n",
    "                \"\"\n",
    "            )\n",
    "            tokens_dict[token_hash] = new_token\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# Function to generate text\n",
    "function generate_text(start_token::String, text_length::Int)\n",
    "    generated_text = [start_token]\n",
    "    current_token = start_token\n",
    "    context = TokenContext(Vector{UInt32}(), 3)\n",
    "    \n",
    "    for _ in 1:text_length\n",
    "        current_token_hash = token_to_hash(current_token)\n",
    "        if haskey(tokens_dict, current_token_hash)\n",
    "            token_info = tokens_dict[current_token_hash]\n",
    "            if !isempty(token_info.following_tokens)\n",
    "                # Consider the context and tf_frequency for the next token selection\n",
    "                next_token_hash = select_next_token(token_info, context)\n",
    "                next_token = tokens_dict[next_token_hash].token\n",
    "                push!(generated_text, next_token)\n",
    "                current_token = next_token\n",
    "                \n",
    "                # Update the context\n",
    "                push!(context.recent_tokens, next_token_hash)\n",
    "                if length(context.recent_tokens) > context.max_length\n",
    "                    popfirst!(context.recent_tokens)\n",
    "                end\n",
    "            else\n",
    "                break\n",
    "            end\n",
    "        else\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return join(generated_text, \" \")\n",
    "end\n",
    "\n",
    "# Function to select the next token considering context and tf_frequency\n",
    "function select_next_token(token_info::Token, context::TokenContext)\n",
    "    # Filter following tokens based on context\n",
    "    filtered_tokens = filter(x -> !(x in context.recent_tokens), token_info.following_tokens)\n",
    "    \n",
    "    # If no tokens left after filtering, use the original following tokens\n",
    "    if isempty(filtered_tokens)\n",
    "        filtered_tokens = token_info.following_tokens\n",
    "    end\n",
    "    \n",
    "    # Select the next token based on tf_frequency\n",
    "    weights = [tokens_dict[x].tf_frequency for x in filtered_tokens]\n",
    "    next_token_index = sample(Weights(weights))\n",
    "    return filtered_tokens[next_token_index]\n",
    "end\n",
    "\n",
    "# Example text\n",
    "text = \"The concept behind ST-GNN is illustrated in Figure 2, where each time step is a graph and is passed through a GCN/GAT network to obtain the resultant encoded graph that embed the inter-relational spatial dependence. Subsequently, these encoded graphs can be modelled exactly like time series data as long as the integrity of the graphical structure of the data at each time step is preserved. Figure 2 demonstrates these two steps, the temporal model could be any sequential model ranging from ARIMA or simple recurrent neural network to transformers.\"\n",
    "\n",
    "# Process the text\n",
    "process_text(text)\n",
    "\n",
    "# Print the tokens dictionary\n",
    "for (hash_code, token) in tokens_dict\n",
    "    println(\"Token Hash: $hash_code\")\n",
    "    println(\"Token: $(token.token)\")\n",
    "    println(\"Preceding Tokens: \", [Int(x) for x in token.preceding_tokens])\n",
    "    println(\"Following Tokens: \", [Int(x) for x in token.following_tokens])\n",
    "    println()\n",
    "end\n",
    "\n",
    "# Generate text starting from a seed token\n",
    "seed_token = \"concept\"\n",
    "generated_text = generate_text(seed_token, 50)\n",
    "println(\"Generated Text: $generated_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SHA, Random\n",
    "\n",
    "# Define the Token struct\n",
    "# struct Token\n",
    "#     hash_code::UInt32\n",
    "#     entity_instances::Vector{Entity.Instance{10}}\n",
    "#     tf_frequency::Float64\n",
    "#     bin_number::UInt16\n",
    "#     trailing_zeros::UInt8\n",
    "#     preceding_tokens::Vector{UInt32}\n",
    "#     following_tokens::Vector{UInt32}\n",
    "#     commit_id::String\n",
    "# end\n",
    "\n",
    "# Initialize a dictionary to store tokens by their hash codes\n",
    "tokens_dict = Dict{UInt32, Token}()\n",
    "\n",
    "# Function to tokenize the text\n",
    "function tokenize(text::String)\n",
    "    return split(text, r\"\\W+\")\n",
    "end\n",
    "\n",
    "# Function to convert a token to a hash\n",
    "function token_to_hash(token::AbstractString)\n",
    "    return reinterpret(UInt32, sha1(String(token))[1:4])\n",
    "end\n",
    "\n",
    "# Function to process the text\n",
    "function process_text(text::String)\n",
    "    tokens = tokenize(text)\n",
    "    num_tokens = length(tokens)\n",
    "    \n",
    "    for i in 1:num_tokens\n",
    "        token = tokens[i]\n",
    "        token_hash = token_to_hash(token)\n",
    "        \n",
    "        if haskey(tokens_dict, token_hash)\n",
    "            # Token already exists, update properties\n",
    "            existing_token = tokens_dict[token_hash]\n",
    "            if i > 1\n",
    "                preceding_token_hash = token_to_hash(tokens[i-1])\n",
    "                push!(existing_token.preceding_tokens, preceding_token_hash)\n",
    "            end\n",
    "            if i < num_tokens\n",
    "                following_token_hash = token_to_hash(tokens[i+1])\n",
    "                push!(existing_token.following_tokens, following_token_hash)\n",
    "            end\n",
    "        else\n",
    "            # Token does not exist, create a new instance\n",
    "            new_token = Token(\n",
    "                token_hash,\n",
    "                Vector{Entity.Instance{10}}(),\n",
    "                1,\n",
    "                rand(UInt16),\n",
    "                0,\n",
    "                i > 1 ? [token_to_hash(tokens[i-1])] : [],\n",
    "                i < num_tokens ? [token_to_hash(tokens[i+1])] : [],\n",
    "                \"\"\n",
    "            )\n",
    "            tokens_dict[token_hash] = new_token\n",
    "        end\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored Text: The concept behind ST GNN is illustrated in Figure 2 where each time step is a graph and is passed through a GCN GAT network to obtain the resultant encoded graph that embed the inter relational spatial dependence Subsequently these encoded graphs can be modelled exactly like time series data as long as the integrity of the graphical structure of the data at each time step is preserved Figure 2 demonstrates these two steps the temporal model could be any sequential model ranging from ARIMA or simple recurrent neural network to transformers \n"
     ]
    }
   ],
   "source": [
    "using SHA, Random, StatsBase\n",
    "\n",
    "# Define the Token struct\n",
    "struct Token\n",
    "    hash_code::UInt32\n",
    "    token::String\n",
    "    tf_frequency::Float64\n",
    "    bin_number::UInt16\n",
    "    trailing_zeros::Int\n",
    "    preceding_tokens::Vector{UInt32}\n",
    "    following_tokens::Vector{UInt32}\n",
    "    commit_id::String\n",
    "    positions::Vector{Int}  # Add positions to store the original positions of the tokens\n",
    "end\n",
    "\n",
    "# Initialize a dictionary to store tokens by their hash codes\n",
    "tokens_dict = Dict{UInt32, Token}()\n",
    "\n",
    "# Function to tokenize the text\n",
    "function tokenize(text::String)\n",
    "    return split(text, r\"\\W+\")\n",
    "end\n",
    "\n",
    "# Function to convert a token to a hash\n",
    "function token_to_hash(token::AbstractString)\n",
    "    return reinterpret(UInt32, sha1(String(token))[1:4])[1]\n",
    "end\n",
    "\n",
    "# Function to process the text\n",
    "function process_text(text::String)\n",
    "    tokens = tokenize(text)\n",
    "    num_tokens = length(tokens)\n",
    "    \n",
    "    for i in 1:num_tokens\n",
    "        token = tokens[i]\n",
    "        token_hash = token_to_hash(token)\n",
    "        \n",
    "        if haskey(tokens_dict, token_hash)\n",
    "            # Token already exists, update properties\n",
    "            existing_token = tokens_dict[token_hash]\n",
    "            push!(existing_token.positions, i)\n",
    "            if i > 1\n",
    "                preceding_token_hash = token_to_hash(tokens[i-1])\n",
    "                push!(existing_token.preceding_tokens, preceding_token_hash)\n",
    "            end\n",
    "            if i < num_tokens\n",
    "                following_token_hash = token_to_hash(tokens[i+1])\n",
    "                push!(existing_token.following_tokens, following_token_hash)\n",
    "            end\n",
    "        else\n",
    "            # Token does not exist, create a new instance\n",
    "            new_token = Token(\n",
    "                token_hash,\n",
    "                token,\n",
    "                1.0,\n",
    "                rand(UInt16),\n",
    "                0,\n",
    "                i > 1 ? [token_to_hash(tokens[i-1])] : [],\n",
    "                i < num_tokens ? [token_to_hash(tokens[i+1])] : [],\n",
    "                \"\",\n",
    "                [i]  # Initialize positions with the current position\n",
    "            )\n",
    "            tokens_dict[token_hash] = new_token\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# Function to retrieve and reorder tokens for a given text\n",
    "function restore_text()\n",
    "    tokens_with_positions = []\n",
    "\n",
    "    for (token_hash, token_info) in tokens_dict\n",
    "        for pos in token_info.positions\n",
    "            push!(tokens_with_positions, (pos, token_info.token))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Sort tokens based on their original positions\n",
    "    sorted_tokens = sort(tokens_with_positions, by = x -> x[1])\n",
    "    return join([token for (_, token) in sorted_tokens], \" \")\n",
    "end\n",
    "\n",
    "# Example text\n",
    "text = \"The concept behind ST-GNN is illustrated in Figure 2, where each time step is a graph and is passed through a GCN/GAT network to obtain the resultant encoded graph that embed the inter-relational spatial dependence. Subsequently, these encoded graphs can be modelled exactly like time series data as long as the integrity of the graphical structure of the data at each time step is preserved. Figure 2 demonstrates these two steps, the temporal model could be any sequential model ranging from ARIMA or simple recurrent neural network to transformers.\"\n",
    "\n",
    "# Process the text\n",
    "process_text(text)\n",
    "\n",
    "# Restore the original text\n",
    "restored_text = restore_text()\n",
    "println(\"Restored Text: $restored_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positions of 'concept': [2]\n"
     ]
    }
   ],
   "source": [
    "# Function to get the positions of a specific token\n",
    "function get_token_positions(token::String)\n",
    "    token_hash = token_to_hash(token)\n",
    "    if haskey(tokens_dict, token_hash)\n",
    "        token_info = tokens_dict[token_hash]\n",
    "        return token_info.positions\n",
    "    else\n",
    "        return \"Token not found\"\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positions of 'is': [6, 15, 19, 67]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "token = \"is\"\n",
    "positions = get_token_positions(token)\n",
    "println(\"Positions of '$token': $positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SHA, Random, StatsBase\n",
    "\n",
    "# Define the Token struct\n",
    "struct Token\n",
    "    hash_code::UInt32\n",
    "    token::String\n",
    "    tf_frequency::Float64\n",
    "    bin_number::UInt16\n",
    "    trailing_zeros::Int\n",
    "    preceding_tokens::Vector{UInt32}\n",
    "    following_tokens::Vector{UInt32}\n",
    "    commit_id::String\n",
    "    positions::Vector{Float64}  # Positions as a normalized histogram\n",
    "end\n",
    "\n",
    "# Initialize a dictionary to store tokens by their hash codes\n",
    "tokens_dict = Dict{UInt32, Token}()\n",
    "\n",
    "# Function to tokenize the text\n",
    "function tokenize(text::String)\n",
    "    return split(text, r\"\\W+\")\n",
    "end\n",
    "\n",
    "# Function to convert a token to a hash\n",
    "function token_to_hash(token::AbstractString)\n",
    "    return reinterpret(UInt32, sha1(String(token))[1:4])[1]\n",
    "end\n",
    "\n",
    "# Function to process the text in chunks\n",
    "function process_text(text::String, chunk_size::Int)\n",
    "    tokens = tokenize(text)\n",
    "    num_tokens = length(tokens)\n",
    "    \n",
    "    for i in 1:chunk_size:num_tokens\n",
    "        chunk = tokens[i:min(i+chunk_size-1, num_tokens)]\n",
    "        for (j, token) in enumerate(chunk)\n",
    "            token_hash = token_to_hash(token)\n",
    "            \n",
    "            if haskey(tokens_dict, token_hash)\n",
    "                # Token already exists, update properties\n",
    "                existing_token = tokens_dict[token_hash]\n",
    "                if j <= length(existing_token.positions)\n",
    "                    existing_token.positions[j] += 1.0\n",
    "                else\n",
    "                    push!(existing_token.positions, 1.0)\n",
    "                end\n",
    "            else\n",
    "                # Token does not exist, create a new instance\n",
    "                positions = zeros(Float64, chunk_size)\n",
    "                positions[j] = 1.0\n",
    "                new_token = Token(\n",
    "                    token_hash,\n",
    "                    token,\n",
    "                    1.0,\n",
    "                    rand(UInt16),\n",
    "                    0,\n",
    "                    j > 1 ? [token_to_hash(chunk[j-1])] : [],\n",
    "                    j < length(chunk) ? [token_to_hash(chunk[j+1])] : [],\n",
    "                    \"\",\n",
    "                    positions\n",
    "                )\n",
    "                tokens_dict[token_hash] = new_token\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Normalize the positions histogram for each token\n",
    "    for (token_hash, token_info) in tokens_dict\n",
    "        total = sum(token_info.positions)\n",
    "        if total > 0\n",
    "            token_info.positions .= token_info.positions ./ total\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# Function to retrieve and reorder tokens for a given text\n",
    "function restore_text()\n",
    "    tokens_with_positions = []\n",
    "\n",
    "    for (token_hash, token_info) in tokens_dict\n",
    "        for (pos, freq) in enumerate(token_info.positions)\n",
    "            if freq > 0\n",
    "                push!(tokens_with_positions, (pos, token_info.token, freq))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Sort tokens based on their original positions\n",
    "    sorted_tokens = sort(tokens_with_positions, by = x -> x[1])\n",
    "    return join([token for (_, token, _) in sorted_tokens], \" \")\n",
    "end\n",
    "\n",
    "# Example text\n",
    "text = \"The concept behind ST-GNN is illustrated in Figure 2, where each time step is a graph and is passed through a GCN/GAT network to obtain the resultant encoded graph that embed the inter-relational spatial dependence. Subsequently, these encoded graphs can be modelled exactly like time series data as long as the integrity of the graphical structure of the data at each time step is preserved. Figure 2 demonstrates these two steps, the temporal model could be any sequential model ranging from ARIMA or simple recurrent neural network to transformers.\"\n",
    "\n",
    "# Process the text in chunks of 11 tokens\n",
    "process_text(text, 11)\n",
    "\n",
    "# Restore the original text\n",
    "restored_text = restore_text()\n",
    "println(\"Restored Text: $restored_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example text\n",
    "text = \"The concept behind ST-GNN is illustrated in Figure 2, where each time step is a graph and is passed through a GCN/GAT network to obtain the resultant encoded graph that embed the inter-relational spatial dependence. Subsequently, these encoded graphs can be modelled exactly like time series data as long as the integrity of the graphical structure of the data at each time step is preserved. Figure 2 demonstrates these two steps, the temporal model could be any sequential model ranging from ARIMA or simple recurrent neural network to transformers.\"\n",
    "\n",
    "# Process the text\n",
    "process_text(text)\n",
    "\n",
    "# Print the tokens dictionary\n",
    "for (hash_code, token) in tokens_dict\n",
    "    println(\"Token Hash: $hash_code\")\n",
    "    println(\"Preceding Tokens: $(token.preceding_tokens)\")\n",
    "    println(\"Following Tokens: $(token.following_tokens)\")\n",
    "    println()\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
